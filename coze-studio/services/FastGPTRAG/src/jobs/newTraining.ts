import { Types } from 'mongoose';
import { MongoDatasetCollection } from '@/core/dataset/collection/schema.js';
import { MongoDatasetData } from '@/core/dataset/data/schema.js';
import { TrainingModeEnum } from '@/types/dataset.js';
import { logger } from '@/utils/logger.js';
import { insertDatasetDataVector, getVectorStore } from '@/core/vectorstore/newController.js';
import { getEmbeddingModel } from '@/core/embedding/index.js';
import { safeObjectId, isValidObjectId } from '@/utils/objectId.js';
import { startQATrainingJob, generateQA } from './qaTraining.js';

export interface TrainingJobParams {
  collectionId: string;
  teamId: string;
  tmbId: string;
  mode: TrainingModeEnum;
  batchSize?: number;
}

export interface TrainingResult {
  trainingId: string;
  status: 'started' | 'completed' | 'failed';
  processedCount: number;
  totalCount: number;
  error?: string;
}

// å…¨å±€è®­ç»ƒé˜Ÿåˆ—è®¡æ•°
declare global {
  var vectorQueueLen: number;
}

if (!(global as any).vectorQueueLen) {
  (global as any).vectorQueueLen = 0;
}

const reduceQueue = () => {
  (global as any).vectorQueueLen = (global as any).vectorQueueLen > 0 ? (global as any).vectorQueueLen - 1 : 0;
  return (global as any).vectorQueueLen === 0;
};

// å¼€å§‹è®­ç»ƒä½œä¸š - å¤ç°åŸç‰ˆFastGPTçš„é˜Ÿåˆ—é€»è¾‘
export async function startTrainingJob(params: TrainingJobParams): Promise<string> {
  const { collectionId, teamId, tmbId, mode, batchSize = 10 } = params;
  const trainingId = new Types.ObjectId().toString();

  try {
    logger.info(`Starting training job: ${trainingId} for collection: ${collectionId}`);

    // éªŒè¯å‚æ•°
    if (!isValidObjectId(collectionId)) {
      throw new Error(`Invalid ObjectId format: collectionId=${collectionId}`);
    }

    const teamIdForQuery = safeObjectId(teamId, '000000000000000000000001');

    // è·å–é›†åˆä¿¡æ¯
    const collection = await MongoDatasetCollection.findOne({
      _id: safeObjectId(collectionId),
      teamId: teamIdForQuery
    }).populate('datasetId');

    if (!collection) {
      throw new Error('Collection not found or access denied');
    }

    // è·å–éœ€è¦è®­ç»ƒçš„æ•°æ®é¡¹
    const dataItems = await MongoDatasetData.find({
      collectionId: safeObjectId(collectionId),
      teamId: teamIdForQuery
    });

    if (dataItems.length === 0) {
      logger.info(`No data items to train in collection: ${collectionId}`);
      return trainingId;
    }

    // æ ¹æ®è®­ç»ƒæ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
    if (mode === TrainingModeEnum.qa) {
      // QAè®­ç»ƒæ¨¡å¼
      await startQATrainingJob({
        collectionId,
        teamId,
        tmbId,
        batchSize,
        qaPrompt: collection.qaPrompt,
        agentModel: collection.datasetId?.agentModel,
        vectorModel: collection.datasetId?.vectorModel
      });
    } else {
      // å…¶ä»–è®­ç»ƒæ¨¡å¼ï¼ˆchunk, auto, imageç­‰ï¼‰
      processTrainingQueue({
        trainingId,
        collectionId,
        dataItems,
        collection,
        mode,
        batchSize
      });
    }

    return trainingId;
  } catch (error) {
    logger.error(`Failed to start training job:`, error);
    throw error;
  }
}

// å¤„ç†è®­ç»ƒé˜Ÿåˆ— - æ¨¡æ‹ŸåŸç‰ˆFastGPTçš„generateVectoré˜Ÿåˆ—
async function processTrainingQueue(params: {
  trainingId: string;
  collectionId: string;
  dataItems: any[];
  collection: any;
  mode: TrainingModeEnum;
  batchSize: number;
}): Promise<void> {
  const { trainingId, collectionId, dataItems, collection, mode, batchSize } = params;

  // æ£€æŸ¥é˜Ÿåˆ—é•¿åº¦
  const maxProcess = 15; // é»˜è®¤æœ€å¤§å¹¶å‘æ•°
  if ((global as any).vectorQueueLen >= maxProcess) {
    logger.info(`Vector queue full, delaying training job: ${trainingId}`);
    setTimeout(() => processTrainingQueue(params), 5000);
    return;
  }

  (global as any).vectorQueueLen++;

  try {
    logger.info(`Processing training queue: ${trainingId}, ${dataItems.length} items`);
    logger.info(`Collection info:`, {
      collectionId: collection._id,
      datasetId: collection.datasetId._id || collection.datasetId,
      vectorModel: collection.datasetId.vectorModel || 'text-embedding-v3'
    });

    // æ ‡è®°æ•°æ®ä¸ºé‡å»ºä¸­ï¼ŒåŒæ—¶æ›´æ–°collectionçŠ¶æ€ä¸ºtraining
    await Promise.all([
      MongoDatasetData.updateMany(
        { collectionId: safeObjectId(collectionId) },
        { rebuilding: true }
      ),
      MongoDatasetCollection.findByIdAndUpdate(
        safeObjectId(collectionId),
        { status: 'training', updateTime: new Date() }
      )
    ]);

    // åˆ†æ‰¹å¤„ç†æ•°æ®
    for (let i = 0; i < dataItems.length; i += batchSize) {
      const batch = dataItems.slice(i, i + batchSize);
      
      try {
        logger.info(`Processing batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(dataItems.length / batchSize)}`);
        logger.info(`Batch items:`, batch.map(item => ({ 
          id: item._id, 
          q: item.q.substring(0, 50) + '...',
          indexes: item.indexes?.length || 0 
        })));
        
        await processBatch({
          batch,
          collection,
          mode
        });
        
        // æ ‡è®°æ‰¹æ¬¡å®Œæˆ
        const batchIds = batch.map(item => item._id);
        await MongoDatasetData.updateMany(
          { _id: { $in: batchIds } },
          { rebuilding: false, updateTime: new Date() }
        );

        logger.info(`âœ… Processed batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(dataItems.length / batchSize)} for training ${trainingId}`);
      } catch (error) {
        logger.error(`âŒ Failed to process batch ${i / batchSize + 1}:`, error);
        
        // æ ‡è®°æ‰¹æ¬¡å¤±è´¥
        const batchIds = batch.map(item => item._id);
        await MongoDatasetData.updateMany(
          { _id: { $in: batchIds } },
          { rebuilding: false }
        );
      }
      
      // æ·»åŠ å°å»¶è¿Ÿé¿å…APIé™æµ
      await new Promise(resolve => setTimeout(resolve, 100));
    }

    // è®­ç»ƒå®Œæˆï¼Œæ›´æ–°collectionçŠ¶æ€ä¸ºready
    await MongoDatasetCollection.findByIdAndUpdate(
      safeObjectId(collectionId),
      { status: 'ready', updateTime: new Date() }
    );
    
    logger.info(`ğŸ‰ Training job completed: ${trainingId}`);
  } catch (error) {
    logger.error(`âŒ Training queue processing failed:`, error);
    
    // æ¸…ç†é‡å»ºæ ‡è®°å¹¶æ›´æ–°collectionçŠ¶æ€ä¸ºfailed
    await Promise.all([
      MongoDatasetData.updateMany(
        { collectionId: safeObjectId(collectionId) },
        { rebuilding: false }
      ),
      MongoDatasetCollection.findByIdAndUpdate(
        safeObjectId(collectionId),
        { status: 'failed', updateTime: new Date() }
      )
    ]);
  } finally {
    reduceQueue();
  }
}

// å¤„ç†å•ä¸ªæ‰¹æ¬¡ - å¤ç°åŸç‰ˆFastGPTçš„æ‰¹æ¬¡å¤„ç†é€»è¾‘
async function processBatch(params: {
  batch: any[];
  collection: any;
  mode: TrainingModeEnum;
}): Promise<void> {
  const { batch, collection, mode } = params;

  try {
    logger.info(`Starting batch processing with ${batch.length} items`);
    
    // å‡†å¤‡è®­ç»ƒæ•°æ®
    const inputs = batch.map(item => {
      // æ ¹æ®é›†åˆè®¾ç½®ç»„åˆæ ‡é¢˜å’Œå†…å®¹
      if (collection.indexPrefixTitle && collection.name) {
        return `${collection.name}\n${item.q}`;
      }
      return item.q;
    });

    // å‡†å¤‡å…ƒæ•°æ®
    const metadata = batch.map((item, index) => {
      // ç¡®ä¿æ¯ä¸ªæ•°æ®é¡¹éƒ½æœ‰æœ‰æ•ˆçš„dataId
      let dataId = item.indexes?.[0]?.dataId;
      if (!dataId) {
        dataId = `${item._id.toString()}_${index}_${Date.now()}`;
        logger.warn(`Generated missing dataId for item ${item._id}: ${dataId}`);
      }
      
      return {
        dataId,
        q: item.q,
        a: item.a || '',
        chunkIndex: item.chunkIndex || index
      };
    });

    logger.info(`Prepared inputs and metadata:`, {
      inputCount: inputs.length,
      firstInput: inputs[0]?.substring(0, 100) + '...',
      metadataCount: metadata.length,
      firstDataId: metadata[0]?.dataId
    });

    // è·å–åµŒå…¥æ¨¡å‹
    const vectorModel = collection.datasetId?.vectorModel || collection.datasetId || 'text-embedding-v3';
    const embeddingModel = getEmbeddingModel(vectorModel);
    
    logger.info(`Using embedding model:`, {
      model: embeddingModel.model,
      provider: embeddingModel.provider
    });

    // æ’å…¥å‘é‡
    const result = await insertDatasetDataVector({
      inputs,
      model: embeddingModel,
      teamId: collection.teamId.toString(),
      datasetId: (collection.datasetId._id || collection.datasetId).toString(),
      collectionId: collection._id.toString(),
      metadata
    });

    logger.info(`âœ… Batch processed successfully: ${inputs.length} items, tokens: ${result.tokens}, vectorIds: ${result.insertIds.length}`);
    
    // éªŒè¯å‘é‡æ˜¯å¦çœŸçš„è¢«æ’å…¥äº† - ä½¿ç”¨æ­£ç¡®çš„å‘é‡ç»´åº¦
    try {
      const vectorStore = await getVectorStore();
      
      // ä½¿ç”¨åˆšæ’å…¥çš„ç¬¬ä¸€ä¸ªå‘é‡çš„å‰å‡ ä¸ªç»´åº¦ä½œä¸ºæµ‹è¯•ï¼ˆç¡®ä¿ç»´åº¦åŒ¹é…ï¼‰
      if (result.insertIds.length > 0) {
        // åˆ›å»ºä¸€ä¸ª1024ç»´çš„æµ‹è¯•å‘é‡ï¼ˆå…¨éƒ¨è®¾ä¸º0.1ï¼‰
        const testVector = new Array(1024).fill(0.1);
        
        const testResult = await vectorStore.searchVectors(
          testVector,
          1,
          {
            teamId: collection.teamId.toString(),
            datasetId: (collection.datasetId._id || collection.datasetId).toString(),
            collectionId: collection._id.toString()
          }
        );
        
        logger.info(`Vector verification: found ${testResult.length} vectors in store for collection ${collection._id}`);
      }
    } catch (verificationError) {
      logger.warn(`Vector verification failed (non-critical):`, verificationError);
      // éªŒè¯å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
    }
    
  } catch (error) {
    logger.error('âŒ Failed to process batch:', error);
    throw error;
  }
}

// è·å–è®­ç»ƒçŠ¶æ€
export async function getTrainingStatus(trainingId: string): Promise<TrainingResult> {
  try {
    // ç®€åŒ–çš„çŠ¶æ€æ£€æŸ¥ - å®é™…åº”è¯¥å­˜å‚¨åœ¨æ•°æ®åº“ä¸­
    return {
      trainingId,
      status: 'completed',
      processedCount: 0,
      totalCount: 0
    };
  } catch (error) {
    logger.error(`Failed to get training status:`, error);
    throw error;
  }
}

// å–æ¶ˆè®­ç»ƒä½œä¸š
export async function cancelTrainingJob(trainingId: string): Promise<void> {
  try {
    logger.info(`Cancelling training job: ${trainingId}`);
    // å®é™…å®ç°åº”è¯¥ä»é˜Ÿåˆ—ä¸­ç§»é™¤ä½œä¸š
  } catch (error) {
    logger.error(`Failed to cancel training job:`, error);
    throw error;
  }
}

// é‡è¯•å¤±è´¥çš„è®­ç»ƒ
export async function retryFailedTraining(collectionId: string): Promise<string> {
  try {
    // æŸ¥æ‰¾å¤±è´¥çš„é¡¹ç›®ï¼ˆæ ‡è®°ä¸ºé‡å»ºä¸­ä½†è¶…æ—¶çš„ï¼‰
    const cutoffTime = new Date(Date.now() - 30 * 60 * 1000); // 30åˆ†é’Ÿå‰
    
    await MongoDatasetData.updateMany(
      {
        collectionId: safeObjectId(collectionId),
        rebuilding: true,
        updateTime: { $lt: cutoffTime }
      },
      { rebuilding: false }
    );

    // å¯åŠ¨æ–°çš„è®­ç»ƒä½œä¸š
    return await startTrainingJob({
      collectionId,
      teamId: 'system',
      tmbId: 'system',
      mode: TrainingModeEnum.chunk
    });
  } catch (error) {
    logger.error('Failed to retry failed training:', error);
    throw error;
  }
}

// å¯åŠ¨è®­ç»ƒé˜Ÿåˆ—ç®¡ç†å™¨ - å¤ç°åŸç‰ˆFastGPTçš„è®­ç»ƒé˜Ÿåˆ—é€»è¾‘
export const startTrainingQueue = (fast?: boolean) => {
  const max = 10; // æœ€å¤§è¿›ç¨‹æ•°

  for (let i = 0; i < (fast ? max : 1); i++) {
    generateQA(); // QAè®­ç»ƒé˜Ÿåˆ—
    generateVector(); // å‘é‡è®­ç»ƒé˜Ÿåˆ—
  }
};

// å¯åŠ¨å‘é‡ç”Ÿæˆé˜Ÿåˆ— - å¤ç°åŸç‰ˆFastGPTçš„generateVectorå‡½æ•°
export async function generateVector(): Promise<void> {
  const max = 15; // æœ€å¤§å¹¶å‘æ•°
  
  if (global.vectorQueueLen >= max) {
    return;
  }

  (global as any).vectorQueueLen++;

  try {
    // æŸ¥æ‰¾å¾…å¤„ç†çš„è®­ç»ƒä»»åŠ¡
    const pendingData = await MongoDatasetData.find({
      rebuilding: { $ne: true },
      'indexes.0': { $exists: true }
    }).populate([
      {
        path: 'datasetId',
        select: 'vectorModel'
      },
      {
        path: 'collectionId',
        select: 'name indexPrefixTitle'
      }
    ]).limit(10);

    if (pendingData.length === 0) {
      return;
    }

    // æŒ‰é›†åˆåˆ†ç»„å¤„ç†
    const collectionGroups = new Map<string, any[]>();
    
    pendingData.forEach(item => {
      const collectionId = item.collectionId._id.toString();
      if (!collectionGroups.has(collectionId)) {
        collectionGroups.set(collectionId, []);
      }
      collectionGroups.get(collectionId)!.push(item);
    });

    // å¤„ç†æ¯ä¸ªé›†åˆçš„æ•°æ®
    for (const [collectionId, items] of collectionGroups) {
      try {
        await processBatch({
          batch: items,
          collection: items[0].collectionId,
          mode: TrainingModeEnum.chunk
        });

        // æ ‡è®°å®Œæˆ
        const itemIds = items.map(item => item._id);
        await MongoDatasetData.updateMany(
          { _id: { $in: itemIds } },
          { rebuilding: false, updateTime: new Date() }
        );

        logger.info(`Vector generation completed for collection: ${collectionId}`);
      } catch (error) {
        logger.error(`Vector generation failed for collection ${collectionId}:`, error);
      }
    }

  } catch (error) {
    logger.error('Vector generation failed:', error);
  } finally {
    reduceQueue();
  }
}
