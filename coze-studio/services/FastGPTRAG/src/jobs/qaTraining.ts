import { Types } from 'mongoose';
import { MongoDatasetCollection } from '@/core/dataset/collection/schema.js';
import { MongoDatasetData } from '@/core/dataset/data/schema.js';
import { TrainingModeEnum } from '@/types/dataset.js';
import { logger } from '@/utils/logger.js';
import { qaTrainingProcessor, type QATrainingRequest } from '@/core/dataset/training/qaTraining.js';
import { safeObjectId, isValidObjectId } from '@/utils/objectId.js';

export interface QATrainingJobParams {
  collectionId: string;
  teamId: string;
  tmbId: string;
  batchSize?: number;
  qaPrompt?: string;
  agentModel?: string;
  vectorModel?: string;
}

export interface QATrainingResult {
  trainingId: string;
  status: 'started' | 'completed' | 'failed';
  processedCount: number;
  totalCount: number;
  generatedQACount: number;
  error?: string;
}

// å…¨å±€QAè®­ç»ƒé˜Ÿåˆ—è®¡æ•°
declare global {
  var qaQueueLen: number;
}

if (!(global as any).qaQueueLen) {
  (global as any).qaQueueLen = 0;
}

const reduceQAQueue = () => {
  (global as any).qaQueueLen = (global as any).qaQueueLen > 0 ? (global as any).qaQueueLen - 1 : 0;
  return (global as any).qaQueueLen === 0;
};

/**
 * å¼€å§‹QAè®­ç»ƒä½œä¸š - å¤ç°åŸç‰ˆFastGPTçš„QAç”Ÿæˆé˜Ÿåˆ—é€»è¾‘
 */
export async function startQATrainingJob(params: QATrainingJobParams): Promise<string> {
  const { 
    collectionId, 
    teamId, 
    tmbId, 
    batchSize = 5, 
    qaPrompt,
    agentModel = 'gpt-3.5-turbo',
    vectorModel = 'text-embedding-v3'
  } = params;
  
  const trainingId = new Types.ObjectId().toString();

  try {
    logger.info(`Starting QA training job: ${trainingId} for collection: ${collectionId}`);

    // éªŒè¯å‚æ•°
    if (!isValidObjectId(collectionId)) {
      throw new Error(`Invalid ObjectId format: collectionId=${collectionId}`);
    }

    const teamIdForQuery = safeObjectId(teamId, '000000000000000000000001');

    // è·å–é›†åˆä¿¡æ¯
    const collection = await MongoDatasetCollection.findOne({
      _id: safeObjectId(collectionId),
      teamId: teamIdForQuery
    }).populate('datasetId');

    if (!collection) {
      throw new Error('Collection not found or access denied');
    }

    // è·å–éœ€è¦è¿›è¡ŒQAè®­ç»ƒçš„æ•°æ®é¡¹
    const dataItems = await MongoDatasetData.find({
      collectionId: safeObjectId(collectionId),
      teamId: teamIdForQuery,
      q: { $exists: true, $ne: '' }
    }).limit(100); // é™åˆ¶æ‰¹æ¬¡å¤§å°

    if (dataItems.length === 0) {
      logger.info(`No data items for QA training in collection: ${collectionId}`);
      return trainingId;
    }

    // å¯åŠ¨å¼‚æ­¥QAè®­ç»ƒè¿›ç¨‹
    processQATrainingQueue({
      trainingId,
      collectionId,
      dataItems,
      collection,
      batchSize,
      qaPrompt,
      agentModel,
      vectorModel,
      teamId,
      tmbId
    });

    return trainingId;
  } catch (error) {
    logger.error(`Failed to start QA training job:`, error);
    throw error;
  }
}

/**
 * å¤„ç†QAè®­ç»ƒé˜Ÿåˆ— - æ¨¡æ‹ŸåŸç‰ˆFastGPTçš„generateQAé˜Ÿåˆ—
 */
async function processQATrainingQueue(params: {
  trainingId: string;
  collectionId: string;
  dataItems: any[];
  collection: any;
  batchSize: number;
  qaPrompt?: string;
  agentModel: string;
  vectorModel: string;
  teamId: string;
  tmbId: string;
}): Promise<void> {
  const { 
    trainingId, 
    collectionId, 
    dataItems, 
    collection, 
    batchSize,
    qaPrompt,
    agentModel,
    vectorModel,
    teamId,
    tmbId
  } = params;

  // æ£€æŸ¥é˜Ÿåˆ—é•¿åº¦
  const maxProcess = 10; // æœ€å¤§å¹¶å‘æ•°
  if ((global as any).qaQueueLen >= maxProcess) {
    logger.info(`QA queue full, delaying training job: ${trainingId}`);
    setTimeout(() => processQATrainingQueue(params), 5000);
    return;
  }

  (global as any).qaQueueLen++;

  try {
    logger.info(`Processing QA training queue: ${trainingId}, ${dataItems.length} items`);
    
    // æ›´æ–°collectionçŠ¶æ€ä¸ºtraining
    await MongoDatasetCollection.findByIdAndUpdate(
      safeObjectId(collectionId),
      { status: 'training', updateTime: new Date() }
    );

    let processedCount = 0;
    let totalGeneratedQAs = 0;

    // åˆ†æ‰¹å¤„ç†æ•°æ®
    for (let i = 0; i < dataItems.length; i += batchSize) {
      const batch = dataItems.slice(i, i + batchSize);
      
      try {
        logger.info(`Processing QA batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(dataItems.length / batchSize)}`);
        
        const batchResult = await processQABatch({
          batch,
          collection,
          qaPrompt,
          agentModel,
          vectorModel,
          teamId,
          tmbId
        });
        
        processedCount += batch.length;
        totalGeneratedQAs += batchResult.totalQAs;

        logger.info(`âœ… Processed QA batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(dataItems.length / batchSize)} for training ${trainingId}`);
        logger.info(`Generated ${batchResult.totalQAs} QA pairs in this batch`);
        
      } catch (error) {
        logger.error(`âŒ Failed to process QA batch ${i / batchSize + 1}:`, error);
      }
      
      // æ·»åŠ å»¶è¿Ÿé¿å…APIé™æµ
      await new Promise(resolve => setTimeout(resolve, 2000));
    }

    // QAè®­ç»ƒå®Œæˆï¼Œæ›´æ–°collectionçŠ¶æ€ä¸ºready
    await MongoDatasetCollection.findByIdAndUpdate(
      safeObjectId(collectionId),
      { status: 'ready', updateTime: new Date() }
    );
    
    logger.info(`ğŸ‰ QA training job completed: ${trainingId}`);
    logger.info(`Processed ${processedCount} items, generated ${totalGeneratedQAs} QA pairs`);
    
  } catch (error) {
    logger.error(`âŒ QA training queue processing failed:`, error);
    
    // æ›´æ–°collectionçŠ¶æ€ä¸ºfailed
    await MongoDatasetCollection.findByIdAndUpdate(
      safeObjectId(collectionId),
      { status: 'failed', updateTime: new Date() }
    );
  } finally {
    reduceQAQueue();
  }
}

/**
 * å¤„ç†QAè®­ç»ƒæ‰¹æ¬¡
 */
async function processQABatch(params: {
  batch: any[];
  collection: any;
  qaPrompt?: string;
  agentModel: string;
  vectorModel: string;
  teamId: string;
  tmbId: string;
}): Promise<{ totalQAs: number }> {
  const { batch, collection, qaPrompt, agentModel, vectorModel, teamId, tmbId } = params;

  try {
    logger.info(`Starting QA batch processing with ${batch.length} items`);
    
    let totalQAs = 0;

    // å¯¹æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ•°æ®é¡¹è¿›è¡ŒQAç”Ÿæˆ
    for (const item of batch) {
      try {
        // æ„å»ºQAè®­ç»ƒè¯·æ±‚
        const qaRequest: QATrainingRequest = {
          text: item.q + (item.a ? '\n' + item.a : ''), // åˆå¹¶é—®é¢˜å’Œç­”æ¡ˆä½œä¸ºè®­ç»ƒæ–‡æœ¬
          datasetId: (collection.datasetId._id || collection.datasetId).toString(),
          collectionId: collection._id.toString(),
          teamId,
          tmbId,
          chunkIndex: item.chunkIndex || 0,
          qaPrompt,
          agentModel,
          vectorModel
        };

        // éªŒè¯è¯·æ±‚
        const validation = qaTrainingProcessor.validateQATrainingRequest(qaRequest);
        if (!validation.valid) {
          logger.warn(`Skipping invalid QA training request: ${validation.error}`);
          continue;
        }

        // å¤„ç†QAè®­ç»ƒ
        const result = await qaTrainingProcessor.processQATraining(qaRequest);
        
        if (result.success) {
          totalQAs += result.qaCount;
          logger.debug(`Generated ${result.qaCount} QA pairs for item ${item._id}`);
        } else {
          logger.warn(`QA training failed for item ${item._id}: ${result.error}`);
        }

      } catch (error) {
        logger.error(`Failed to process QA for item ${item._id}:`, error);
      }

      // æ·»åŠ å°å»¶è¿Ÿé¿å…APIè¿‡è½½
      await new Promise(resolve => setTimeout(resolve, 500));
    }

    logger.info(`âœ… QA batch processed successfully: ${batch.length} items, ${totalQAs} QA pairs generated`);
    return { totalQAs };
    
  } catch (error) {
    logger.error('âŒ Failed to process QA batch:', error);
    throw error;
  }
}

/**
 * å¯åŠ¨QAç”Ÿæˆé˜Ÿåˆ— - å¤ç°åŸç‰ˆFastGPTçš„generateQAå‡½æ•°
 */
export async function generateQA(): Promise<void> {
  const max = 10; // æœ€å¤§å¹¶å‘æ•°
  
  if ((global as any).qaQueueLen >= max) {
    return;
  }

  (global as any).qaQueueLen++;

  try {
    logger.debug(`[QA Queue] Queue size: ${(global as any).qaQueueLen}`);

    // æŸ¥æ‰¾éœ€è¦QAè®­ç»ƒçš„é›†åˆ
    const collections = await MongoDatasetCollection.find({
      trainingType: 'qa',
      status: { $in: ['pending', 'ready'] }
    }).populate('datasetId').limit(5);

    if (collections.length === 0) {
      return;
    }

    logger.info(`Found ${collections.length} collections for QA training`);

    // å¤„ç†æ¯ä¸ªé›†åˆ
    for (const collection of collections) {
      try {
        await startQATrainingJob({
          collectionId: collection._id.toString(),
          teamId: collection.teamId.toString(),
          tmbId: collection.tmbId.toString(),
          qaPrompt: collection.qaPrompt,
          agentModel: collection.datasetId?.agentModel || 'gpt-3.5-turbo',
          vectorModel: collection.datasetId?.vectorModel || 'text-embedding-v3'
        });

        logger.info(`QA training started for collection: ${collection._id}`);
      } catch (error) {
        logger.error(`QA training failed for collection ${collection._id}:`, error);
      }
    }

  } catch (error) {
    logger.error('QA generation queue failed:', error);
  } finally {
    reduceQAQueue();
  }
}

/**
 * è·å–QAè®­ç»ƒçŠ¶æ€
 */
export async function getQATrainingStatus(trainingId: string): Promise<QATrainingResult> {
  try {
    // ç®€åŒ–çš„çŠ¶æ€æ£€æŸ¥ - å®é™…åº”è¯¥å­˜å‚¨åœ¨æ•°æ®åº“ä¸­
    return {
      trainingId,
      status: 'completed',
      processedCount: 0,
      totalCount: 0,
      generatedQACount: 0
    };
  } catch (error) {
    logger.error(`Failed to get QA training status:`, error);
    throw error;
  }
}

/**
 * å–æ¶ˆQAè®­ç»ƒä½œä¸š
 */
export async function cancelQATrainingJob(trainingId: string): Promise<void> {
  try {
    logger.info(`Cancelling QA training job: ${trainingId}`);
    // å®é™…å®ç°åº”è¯¥ä»é˜Ÿåˆ—ä¸­ç§»é™¤ä½œä¸š
  } catch (error) {
    logger.error(`Failed to cancel QA training job:`, error);
    throw error;
  }
}

/**
 * ä¼°ç®—QAè®­ç»ƒæˆæœ¬
 */
export async function estimateQATrainingCost(collectionId: string): Promise<{
  estimatedCost: number;
  estimatedTime: number;
  estimatedQACount: number;
}> {
  try {
    // è·å–é›†åˆä¸­çš„æ•°æ®é‡
    const dataCount = await MongoDatasetData.countDocuments({
      collectionId: safeObjectId(collectionId)
    });

    // ä¼°ç®—å¹³å‡æ–‡æœ¬é•¿åº¦
    const avgTextLength = 500; // å‡è®¾å¹³å‡500å­—ç¬¦
    
    // ä½¿ç”¨QAè®­ç»ƒå¤„ç†å™¨ä¼°ç®—æˆæœ¬
    const costEstimate = qaTrainingProcessor.estimateQATrainingCost(
      dataCount * avgTextLength
    );

    return {
      estimatedCost: costEstimate.estimatedCost,
      estimatedTime: dataCount * 3, // æ¯ä¸ªæ•°æ®é¡¹çº¦3ç§’
      estimatedQACount: costEstimate.estimatedQACount
    };
  } catch (error) {
    logger.error('Failed to estimate QA training cost:', error);
    throw error;
  }
}
